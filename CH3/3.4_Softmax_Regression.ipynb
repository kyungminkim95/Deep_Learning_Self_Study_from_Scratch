{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Regression\n",
    "# Vectorization for Minibatches\n",
    "# We can think that broadcasting is conducted to the row vector b\n",
    "# For each row of O, exponentiate all entries and then normalize them by the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprisal\n",
    "# The entropy can be understooad as the 'expected surprisal' when one assigned the correct probabilities that truly match the data-generating process.\n",
    "# The cross-entropy from P to Q, denoted H(P,Q), is the 'expected surprisal' of an observer with subjective probabilities Q (estimated proability y_hat의 negative logarithm으로 이해하면 될 것 같습니다.) upon seeing data \n",
    "# that were actually generated according to probabilities P (y로 이해하면 될 것 같습니다. 일종의 degenerated probability distribution으로서). \n",
    "# The lowest possible cross-entropy is achieved when P=Q.\n",
    "# In short, we can think of the cross-entropy classification objective in two ways:\n",
    "# (1) maximum likelihood estimation\n",
    "# (2) minimization of the surprisal (=equivalent to reach the entropy / surprisal given true distn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
