{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-04 12:39:29.699382: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Softmax Regression from Scratch\n",
    "import tensorflow as tf\n",
    "from IPython import display\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "# Loading the dataset and partitioning the dataset into minibatches\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Model Parameters\n",
    "# In this section, we will flatten each image, treating them as vectors of length 784.\n",
    "# In the future, we will talk about more sophisticated strategies for exploiting the spatial structure in images. (I think they are talking about the CNN.)\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "# Our weights will constitute a 784 * 10 matrix and the biases will constitute a 1 * 10 row vector.\n",
    "# As with linear regression, we will initialize weights 'w' with Gaussian noise and our biases to take the initial value 0.\n",
    "W = tf.Variable(tf.random.normal(shape = (num_inputs, num_outputs),\n",
    "mean=0, stddev=0.01))\n",
    "b = tf.Variable(tf.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[5., 7., 9.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       " array([[ 6.],\n",
       "        [15.]], dtype=float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the Softmax Operation\n",
    "# 'keepdims=True' option pertains the original structure of the object that is used for the operation.\n",
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "tf.reduce_sum(X, 0, keepdims=True), tf.reduce_sum(X, 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The operation seems to be made on the 2-dimensional object.\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X) # exponentiate all the values\n",
    "    partition = tf.reduce_sum(X_exp, 1, keepdims=True) # It can be understood as derivation of the normalizing constant.\n",
    "    return X_exp / partition # The broadcasting mechanism is applied here. I think it can be the reason why we made the option 'keepdims=True'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       " array([[0.40271994, 0.04838905, 0.27165487, 0.13820343, 0.1390327 ],\n",
       "        [0.27239957, 0.05196008, 0.4103868 , 0.14165089, 0.12360267]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.random.normal((2, 5), 0, 1) # So, in this case, it can be understood as O matrix in the website. So the classification problem with 5 outputs and 2 observations are here.\n",
    "X_prob = softmax(X)\n",
    "X_prob, tf.reduce_sum(X_prob, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model\n",
    "def net(X):\n",
    "    return softmax(tf.matmul(tf.reshape(X, (-1, W.shape[0])), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.1, 0.5], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the Loss Function\n",
    "# This code chunk is for understanding the role of function 'boolean_mask()'.\n",
    "y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = tf.constant([0, 2])\n",
    "tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1])) # recall that y_hat is of shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.3025851, 0.6931472], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -tf.math.log(tf.boolean_mask(\n",
    "        y_hat, tf.one_hot(y, depth=y_hat.shape[-1])\n",
    "    ))\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Accuracy\n",
    "# The classification accuracy is the fraction of all predictions that are correct.\n",
    "# Although it can be difficult to optimize accuracy directly, \n",
    "# it is often the performance measure that we care most about.\n",
    "def accuracy(y_hat, y): #@save\n",
    "    \"\"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: # when {the number of obs is more than 1} & {output with multicategories}\n",
    "        y_hat = tf.argmax(y_hat, axis=1) # Aha. Now I got it. It returns the 'INDEX' with the largest value across axes of a tensor.  \n",
    "    cmp = tf.cast(y_hat, y.dtype) == y\n",
    "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator: #@save\n",
    "    \"\"\"\"For accumulating sums over 'n' variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter): #@save\n",
    "    \"\"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = Accumulator(2) # No. of correct predictions, no. of predictions\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), d2l.size(y))\n",
    "        #break\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0976"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# First, we define a ftn to train for one epoch.\n",
    "# 이미 parameter initialization은 이루어졌다고 가정하고 있는 function일지도 모르겠습니다. (코드 이해하기 전 생각)\n",
    "# dataset -> into minibatches ('train_iter') -> parameter(s) initialization -> model construction ('net') ->  loss function selection ('loss') -> optimization method ('updater')\n",
    "def train_epoch_ch3(net, train_iter, loss, updater): #@save\n",
    "    \"\"\"\"The training loop defined in Chapter 3.\"\"\"\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = net(X)\n",
    "            # Keras implementations for loss takes (labels, predictions)\n",
    "            # instead of (predictions, labels) that users might implement\n",
    "            # in this book, e.g. 'cross_entropy' that we implemented above\n",
    "            if isinstance(loss, tf.keras.losses.Loss):\n",
    "                l = loss(y, y_hat) # 그냥 이게 정의여서 그렇습니다 (tf.keras.losses.Loss에 정의된 loss들이 그렇게 정의되어있다는 의미).\n",
    "            else:\n",
    "                l = loss(y_hat, y)\n",
    "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
    "            params = net.trainable_variables\n",
    "            grads = tape.gradient(l, params)\n",
    "            updater.apply_gradients(zip(grads, params))\n",
    "        else:\n",
    "            # 이 부분은 아랫부분을 보면서 이해해야 할 것 같습니다.\n",
    "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
    "        # Keras LOSS BY DEFAULT RETURNS THE AVERAGE LOSS IN A BATCH\n",
    "        l_sum = l * float(tf.size(y)) if isinstance(\n",
    "            loss, tf.keras.losses.Loss\n",
    "        ) else tf.reduce_sum(l)\n",
    "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
    "    # Return training loss and training accuracy (can be understood as THEIR RESPECTIVE SAMPLE MEANS)\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before showing the implementation of the training function, we define a utility class that plot data in animation.\n",
    "class Animator: #@save\n",
    "    \"\"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, \n",
    "    xscale='linear', yscale='linear', \n",
    "    fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5,2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: d2l.set.axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend\n",
    "        )\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt) \n",
    "        self.config_axes()       \n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = d2l.plt.subplots(2,2)\n",
    "#axs[0].shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
