{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook을 실행한 브라우저에서 바로 그림을 볼수 있도록\n",
    "%matplotlib inline\n",
    "import random # used for 1) generation of synthetic data or 2) initializations of model parameters\n",
    "import time # d2l에 들어있다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display # d2l에 들어있다.\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer() ftn for calculating the time spent for a given operation\n",
    "class Timer(): #@save\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return tf.reduce_sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return tf.reduce_sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist() # 리스트 형식은 연산이 되지 않아서 한번 변환이 이루어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function for plotting\n",
    "def use_svg_display(): #@save\n",
    "    \"\"\"Use the svg format to display a plot in the Jupyter.\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)): #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid() # 그리드 선을 구성하십시오. 격자무늬를 의미하는 것 같습니다.\n",
    "\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "ylim=None, xscale='linear', yscale='linear',\n",
    "fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "    \n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axis else d2l.plt.gca() # 'd2l.plt.gca()'로 현재의 axes 객체를 구할 수 있습니다.\n",
    "\n",
    "    # Return True if 'X' (tensor or list) has 1 axis.\n",
    "    def has_one_axis(X):\n",
    "        return(hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) \n",
    "        and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X] # It has len(X) = 1 after an operation.\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y) # Same support임을 guarantee 해줍니다.\n",
    "    axes.cla() # 현재의 좌표축을 지웁니다.\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt) # 얘는 일종의 방어적 프로그래밍으로 이해할 수 있다고 생각합니다.\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Dataset\n",
    "# w : tensor of weight\n",
    "# b : bias (length 1 - since linear regression setting)\n",
    "# num_examples : number of (training) samples to generate\n",
    "def synthetic_data(w, b, num_examples): #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0])) # 'w.shape[0]'을 하는 이유는 w가 tensor이기 때문입니다. 궁금하다면 관련 코드를 직접 작성하여 실행해보면 될 것 같습니다.\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1,1))) + b # Regression function. 왜 'tf.reshape()'을 사용하는지는 'w.shape()'를 한번 사용해보면 알 수 있습니다.\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the Dataset into Minibatches\n",
    "# Training set이 아닌 경우에는 shuffling이 불필요하다고 생각하는 것 같습니다.\n",
    "# data_arrays : tuple of (features, labels) \n",
    "# batch_size : size of the batch\n",
    "# is_train : if True, then shuffle.\n",
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"Construct a Tensorflow data iterator.\"\"\" # yield문을 이용해서 generator를 반환하는 것과 유사한 / 동일한 목적입니다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    return dataset # Now, the time for iterator to do his/her work (e.g. next(iter(data_iter))). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model ('net')\n",
    "# There exist two versions : \n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.Sequential()에 layer을 addition함으로써 : linear model도 accommodate 가능함을 기억할 필요가 있습니다.\n",
    "# 이 code chunk에는 1)에 대응되는 user-defined ftns만을 정리하여 제공할 계획입니다.\n",
    "# 이에 따라, 2)의 경우에는 원형이 되는 prototype만을 아래와 같이 주석으로 제공하도록 하겠습니다.\n",
    "# 추가적으로, 2)의 경우에는 initialization of model parameters for training도 함께 가능하도록 작성할 수 있습니다.\n",
    "\"\"\"\n",
    "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\"\"\"\n",
    "\n",
    "def linreg(X, w, b): #@save\n",
    "    # The linear regression model.\n",
    "    return tf.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Loss Function ('loss')\n",
    "# There exist two versions :\n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.losses에 속한 loss function들 중 하나를 이용하는 방법이 있습니다.\n",
    "\"\"\"\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\"\"\"\n",
    "\n",
    "def squared_loss(y_hat, y): #@save\n",
    "    # Squared loss.\n",
    "    return (y_hat - tf.reshape(y, y_hat.reshape)) ** 2 / 2 # sum이 아닌 vector 형식으로 주어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Optimization Algorithm\n",
    "# There exist two versions :\n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.optimizers.OPTIMIZER(learning_rate=0.03)\n",
    "\"\"\"\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "\"\"\"\n",
    "\n",
    "# params: tuple of weight and bias\n",
    "# grads: tuple of gradients w.r.t weight and bias, respectively\n",
    "# lr: learning rate\n",
    "# batch_size: size of the batch\n",
    "def sgd(params, grads, lr, batch_size): #@save\n",
    "    # Minibatch stochastic gradient descent\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
