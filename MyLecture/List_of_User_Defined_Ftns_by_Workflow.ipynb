{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook을 실행한 브라우저에서 바로 그림을 볼수 있도록\n",
    "%matplotlib inline\n",
    "#import random # used for 1) generation of synthetic data or 2) initializations of model parameters\n",
    "import time # d2l에 들어있다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display # d2l에 들어있다.\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer() ftn for calculating the time spent for a given operation\n",
    "class Timer(): #@save\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return tf.reduce_sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return tf.reduce_sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist() # 리스트 형식은 연산이 되지 않아서 한번 변환이 이루어졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function for plotting\n",
    "def use_svg_display(): #@save\n",
    "    \"\"\"Use the svg format to display a plot in the Jupyter.\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)): #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid() # 그리드 선을 구성하십시오. 격자무늬를 의미하는 것 같습니다.\n",
    "\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "ylim=None, xscale='linear', yscale='linear',\n",
    "fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "    \n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axis else d2l.plt.gca() # 'd2l.plt.gca()'로 현재의 axes 객체를 구할 수 있습니다.\n",
    "\n",
    "    # Return True if 'X' (tensor or list) has 1 axis.\n",
    "    def has_one_axis(X):\n",
    "        return(hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) \n",
    "        and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X] # It has len(X) = 1 after an operation.\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y) # Same support임을 guarantee 해줍니다.\n",
    "    axes.cla() # 현재의 좌표축을 지웁니다.\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt) # 얘는 일종의 방어적 프로그래밍으로 이해할 수 있다고 생각합니다.\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# User-defined ftn transferring numeric labels into text labels\n",
    "# labels: an array of numerical labels\n",
    "# !!!!! It is used to make title arguments in 'titles' option in 'show_images()' ftn !!!!!\n",
    "def get_fashion_mnist_labels(labels): #@save\n",
    "    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n",
    "    text_labels = [\n",
    "        't-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "    ]\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "# User-defined ftn to visualize the examples.\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5): #@save\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows, * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    # 다차원 배열을 1차원 배열로 만들어주는 함수로 이해하면 될 것 같습니다.\n",
    "    # 아래에서 진행될 iteration을 보다 용이하게 하기 위해서, 'flatten()' 함수를 사전에 사용하는 것 같습니다.\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        # 왜 '.numpy()'로 접근을 하는 것인지는 'imgs' argument로 들어가는 object를 확인\n",
    "        # 그렇지 않으면, 'array'가 아닌 'tensor'이기 때문입니다.\n",
    "        ax.imshow(img.numpy())\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i]) # 'enumerate()' ftn을 사용한 이유입니다.\n",
    "        return axes     \n",
    "\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "# Defining a utility class that plot data in animation.\n",
    "class Animator: #@save\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, \n",
    "    ylim=None, xscale='linear', yscale='linear',\n",
    "    fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "    figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend =[]\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda ftn to capture arguments\n",
    "        self.config_axes = d2l.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xli, ylim , xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y))\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the Dataset\n",
    "# w : tensor of weight\n",
    "# b : bias (length 1 - since linear regression setting)\n",
    "# num_examples : number of (training) samples to generate\n",
    "def synthetic_data(w, b, num_examples): #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0])) # 'w.shape[0]'을 하는 이유는 w가 tensor이기 때문입니다. 궁금하다면 관련 코드를 직접 작성하여 실행해보면 될 것 같습니다.\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1,1))) + b # Regression function. 왜 'tf.reshape()'을 사용하는지는 'w.shape()'를 한번 사용해보면 알 수 있습니다.\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the Dataset into Minibatches\n",
    "# Training set이 아닌 경우에는 shuffling이 불필요하다고 생각하는 것 같습니다.\n",
    "# data_arrays : tuple of (features, labels) \n",
    "# batch_size : size of the batch\n",
    "# is_train : if True, then shuffle.\n",
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"Construct a Tensorflow data iterator.\"\"\" # yield문을 이용해서 generator를 반환하는 것과 유사한 / 동일한 목적입니다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    return dataset # Now, the time for iterator to do his/her work (e.g. next(iter(data_iter))). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!! FOR CLASSIFICATION !!!!!\n",
    "# Loading and Partitioning the Training and Test Datasets into Minibatches\n",
    "# Returns the iterator for memory-efficiency\n",
    "def load_data_fashion_mnist(batch_size, resize=None): #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    mnist_train, mnist_test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    # Divide all numbers by 255 so that all pixel values are between\n",
    "    # 0 and 1, add a batch dimension at the last (one for channel 의미하는 듯).\n",
    "    # And cast label to int32.\n",
    "    process = lambda X, y: (\n",
    "        tf.expand_dims(X, axis=3) / 255, tf.cast(y, dtype='int32')\n",
    "    )\n",
    "    resize_fn = lambda X, y: (\n",
    "        tf.image.resize_with_pad(X, resize, resize) if resize else X, y\n",
    "    )\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(\n",
    "            batch_size).shuffle(len(mnist_train[0])).map(resize_fn),\n",
    "        tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(\n",
    "            batch_size).map(resize_fn)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Model ('net')\n",
    "# There exist two versions : \n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.Sequential()에 layer을 addition함으로써 : linear model도 accommodate 가능함을 기억할 필요가 있습니다.\n",
    "# 이 code chunk에는 1)에 대응되는 user-defined ftns만을 정리하여 제공할 계획입니다.\n",
    "# 이에 따라, 2)의 경우에는 원형이 되는 prototype만을 아래와 같이 주석으로 제공하도록 하겠습니다.\n",
    "# 추가적으로, 2)의 경우에는 initialization of model parameters for training도 함께 가능하도록 작성할 수 있습니다.\n",
    "\"\"\"\n",
    "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\"\"\"\n",
    "\n",
    "def linreg(X, w, b): #@save\n",
    "    # The linear regression model.\n",
    "    return tf.matmul(X, w) + b\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "weight_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01)\n",
    "ned.add(tf.keras.layers.Dense(10, kernel_initializer=weight_initializer))\n",
    "\"\"\"\n",
    "\n",
    "# Defining the 'softmax()' operation.\n",
    "def softmax(X):\n",
    "    X_exp = tf.exp(X)\n",
    "    partition = tf.reduce_sum(X, axis=1, keepdims=True)\n",
    "    return X_exp / partition # The broadcasting mechanism is applied here, thx to 'keepdims=True' option.\n",
    "\n",
    "# Defining the Model\n",
    "def net(X):\n",
    "    return softmax(tf.matmul(tf.reshape(X, (-1, W.shape[0])), W) + b)\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "# Defining the 'relu()' activation function.\n",
    "def relu(X):\n",
    "    return tf.math.maximum(X, 0)\n",
    "\n",
    "# MLP w/ single hidden layer\n",
    "def net(X):\n",
    "    X = tf.reshape(X, (-1, num_inputs))\n",
    "    H = relu(tf.matmul(X, W1) + b1)\n",
    "    return tf.matmul(H, W2) + b2\n",
    "\n",
    "# MLP w/ two hidden layers\n",
    "def net(X):\n",
    "    X = tf.reshape(X, (-1, num_inputs))\n",
    "    H1 = relu(tf.matmul(X, W1) + b1)\n",
    "    H2 = relu(tf.matmul(H1, W2) + b2)\n",
    "    return tf.matmul(H2, W3) + b3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Loss Function ('loss')\n",
    "# There exist two versions :\n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.losses에 속한 loss function들 중 하나를 이용하는 방법이 있습니다.\n",
    "\"\"\"\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\"\"\"\n",
    "\n",
    "def squared_loss(y_hat, y): #@save\n",
    "    # Squared loss.\n",
    "    return (y_hat - tf.reshape(y, y_hat.reshape)) ** 2 / 2 # sum이 아닌 vector 형식으로 주어집니다.\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\"\"\"\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -tf.math.log(tf.boolean_mask(\n",
    "        y_hat, tf.one_hot(y, depth=y_hat.shape[-1])))\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\"\"\"\n",
    "\n",
    "# Loss function\n",
    "def loss(y_hat, y):\n",
    "    return tf.losses.sparse_categorical_crossentropy(\n",
    "        y, y_hat, from_logits=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Optimization Algorithm\n",
    "# There exist two versions :\n",
    "# 1) Manually made type\n",
    "# 2) tf.keras.optimizers.OPTIMIZER(learning_rate=0.03)\n",
    "\"\"\"\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "\"\"\"\n",
    "\n",
    "# params: tuple of weight and bias\n",
    "# grads: tuple of gradients w.r.t weight and bias, respectively\n",
    "# lr: learning rate\n",
    "# batch_size: size of the batch\n",
    "def sgd(params, grads, lr, batch_size): #@save\n",
    "    # Minibatch stochastic gradient descent\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=.1)\n",
    "\"\"\"\n",
    "\n",
    "class Updater(): #@save\n",
    "    \"\"\"For updating parameters using minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, batch_size, grads):\n",
    "        d2l.sgd(self.params, grads, self.lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# Classification Accuracy\n",
    "def accuracy(y_hat, y): #@save\n",
    "    \"\"\"Compute the total number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: # 첫 번째 argument의 의미 잘 이해하자.\n",
    "        y_hat = tf.argmax(y_hat, axis=1)\n",
    "    cmp = tf.cast(y_hat, dtype=y.dtype) == y\n",
    "    return float(tf.reduce_sum(tf.cast(cmp, dtype=y.dtype)))\n",
    "\n",
    "# 'Accumulator()' is a utility class to accumulate sums over multiple variables.\n",
    "class Accumulator: #@save\n",
    "    \"\"\"For accumulating sums over 'n' variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Evaluate the accuracy for any model 'net' on a dataset\n",
    "# that is accessed via the data iterator 'data_iter'.\n",
    "def evaluate_accuracy(net, data_iter): #@Save\n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = Accumulator(2) # No. of correct predictions, no. of predictions\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), d2l.size(y))\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training in classification case can be done using this single module.\n",
    "# 1. data set into minibatches (iterator) / 2. parameter initialization / \n",
    "# 3. net / 4. loss / 5. updater / 6. learning rate / 7. number of epochs\n",
    "def train_epoch_ch3(net, train_iter, loss, updater): #@save\n",
    "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = net(X)\n",
    "            # Keras implementation for loss takes (labels, predictions)\n",
    "            # instead of (predictions, labels) that users might implement\n",
    "            # in this book , e.g. 'cross_entropy()' that we implemented above.\n",
    "            if isinstance(loss, tf.keras.losses.Loss):\n",
    "                l = loss(y, y_hat)\n",
    "            else:\n",
    "                l = loss(y_hat, y)\n",
    "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
    "            params = net.trainable_variables # 잘 생각해보면 keras를 사용할 경우 따로 object를 정의할 필요가 없기 때문입니다.\n",
    "            grads = tape.gradient(l, params) # tuple of weights and biases\n",
    "            updater.apply_gradients(zip(grads, params))\n",
    "        else:\n",
    "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
    "        # Keras loss by default returns the average loss in a batch\n",
    "        l_sum = l * float(tf.size(y)) if isinstance(\n",
    "            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
    "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
    "    # Return training loss and training accuracy (sample mean version)\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "# !!!!!!!!!! WHAT WE USE IS THIS ONE !!!!!!!!!!\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater): #@save\n",
    "    \"\"\"Train a model (defined in Chapter 3).\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "    legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    # 다음의 세줄은 training 및 testing이 잘 이루어졌는지 확인해주는 코드로서,\n",
    "    # 그렇지 않을 시에 대응되는 값을 뱉어내는 함수로 이해할 수 있습니다.\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict_ch3(net, test_iter, n=6): #@save\n",
    "    \"\"\"Predict labels (defined in Chapter 3).\"\"\"\n",
    "    for X, y in test_iter:\n",
    "        break\n",
    "    trues = d2l.get_fashion_mnist_labels(y)\n",
    "    preds = d2l.get_fashion_mnist_labels(tf.argmax(net(X), axis=1))\n",
    "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
    "    d2l.show_images(\n",
    "        tf.reshape(X[0:n], (n, 28, 28)), 1, n, titles=titles[0:n])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
