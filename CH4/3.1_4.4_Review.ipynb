{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook을 실행한 브라우저에서 바로 그림을 볼 수 있도록.\n",
    "%matplotlib inline\n",
    "#import math\n",
    "import random # used for 1) generation of synthetic data or 2) initialization of model parameters \n",
    "import time # d2l에 들어있다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython import display # d2l에 들어있다.\n",
    "from d2l import tensorflow as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization for Speed\n",
    "\"\"\"\n",
    "n = 10000\n",
    "a = tf.ones(n)\n",
    "b = tf.ones(n)\n",
    "\"\"\"\n",
    "\n",
    "class Timer: #@save\n",
    "    \"\"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"\"Return the average time.\"\"\"\n",
    "        return tf.reduce_sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"\"Return the sum of time.\"\"\"\n",
    "        return tf.reduce_sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist() # 리스트 형식은 연산이 되지 않아서 이런 것 같습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c = tf.Variable(tf.zeros(n)) # 'tf.Variable()' is a resource variable. I think it is for memory efficiency.\n",
    "timer = Timer()\n",
    "for i in range(n):\n",
    "    c[i].assign(a[i] + b[i]) # ResourceVariable object does not support item assignment\n",
    "f'{timer.stop():.5f} sec'\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "timer.start()\n",
    "d = a + b\n",
    "f'{timer.stop():.5f} sec'\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def normal(x, mu, sigma):\n",
    "    p = 1 / math.sqrt(2 * math.pi * sigma**2)\n",
    "    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)\n",
    "\n",
    "# Use numpy again for visualization\n",
    "x = np.arange(-7, 7, 0.01)\n",
    "\n",
    "# Mean and standard deviation pairs\n",
    "params = [(0, 1), (0, 2), (3, 1)]\n",
    "d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',\n",
    "ylabel='p(x)', figsize=(4.5, 2.5),\n",
    "legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined functions for plotting\n",
    "def use_svg_display(): #@save\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    display.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)): #@save\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid() # 그리드 선을 구성하십시오. 격자무늬를 의미하는 것 같습니다.\n",
    "\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "ylim=None, xscale='linear', yscale='linear',\n",
    "fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"Plot data points.\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca() # 'd2l.plt.gca()'로 현재의 axes 객체를 구할 수 있습니다.\n",
    "\n",
    "    # Return True if 'X' (tensor or list) has 1 axis\n",
    "    def has_one_axis(X):\n",
    "        return(hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "        and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X] # It has len(X) = 1.\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla() # 현재의 좌표축을 지웁니다.\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt) # 얘는 일종의 방어적 프로그래밍으로 이해할 수 있다고 생각한다.\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "d2l.set_figsize()\n",
    "# The semicolon is for displaying the plot only.\n",
    "d2l.plt.scatter(features[:, 1].numpy(), labels.numpy(), 1); # The last argument determines the size of respective pts.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000241\n",
      "epoch 2, loss 0.000101\n",
      "epoch 3, loss 0.000101\n",
      "error in estimating w tf.Tensor([-0.00021219 -0.0007658 ], shape=(2,), dtype=float32)\n",
      "error in estimating b [-0.0006361]\n"
     ]
    }
   ],
   "source": [
    "# Generating the Dataset\n",
    "def synthetic_data(w, b, num_examples): #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0]))\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b # Regression function. 왜 'reshape()'을 사용하는지는 한번만 출력해보면 알 수 있다.\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)    \n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y\n",
    "\n",
    "# Parameter Initialization\n",
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 4.2\n",
    "#features, labels = synthetic_data(true_w, true_b, num_examples=1000)\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, num_examples=1000)\n",
    "\n",
    "#print('features:', features[0], '\\nlabel:', labels[0]) # Can be understood as part of EDA.\n",
    "\n",
    "# Reading the Dataset\n",
    "# Partitioning the dataset into minibatches\n",
    "def load_array(data_arrays, batch_size, is_train=True): #@save\n",
    "    \"\"\"\"Construct a Tensorflow data iterator.\"\"\" # 'yield문을 이용해서 generator를 반환하는 것과 유사한 목적입니다.'\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "#next(iter(data_iter))\n",
    "\n",
    "\"\"\"\n",
    "# Prototype of partitioning into minibatches\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features) # size of the sample size\n",
    "    indices = list(range(num_examples)) # form of [0, 1, ..., (n-1)]\n",
    "    # The examples are read at random, in no particular order\n",
    "    random.shuffle(indices) # shuffled list of indices of length 'num_examples'\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = tf.constant(indices[i:min(i + batch_size, num_examples)]) # 'min()' is used for the last batch.\n",
    "        yield tf.gather(features, j), tf.gather(labels, j) # generator를 반환합니다. Memory-efficient하다는 점에서 강점이 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "batch_size = 10\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break\n",
    "\"\"\"\n",
    "\n",
    "# Defining the Model and Initializing Model Parameters\n",
    "# 'keras' is the high-level API for TensorFlow.\n",
    "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initializing Model Parameters\n",
    "# !!!!! Note 'tf.GradientTape()' watches 'trainable=True' variables by default !!!!!\n",
    "# !!!!! Weights랑 bias(es)는 무조건 'trainable'한 variable로 설정하는 것이 맞겠다 !!!!!\n",
    "w = tf.Variable(tf.random.normal(shape=(2,1), mean=0, stddev=0.01), trainable=True) # 값 변경이 가능한 '변수'로 이해할 수 있다.\n",
    "b = tf.Variable(tf.zeros(1), trainable = True)\n",
    "\n",
    "# Defining the Model\n",
    "def linreg(X, w, b): #@save\n",
    "    # The linear regression model.\n",
    "    return tf.matmul(X, w) + b\n",
    "\"\"\"    \n",
    "\n",
    "# Defining the Loss Function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\"\"\"\n",
    "# Defining the Loss Function\n",
    "def squared_loss(y_hat, y): #@save\n",
    "    # Squared loss.\n",
    "    return (y_hat - tf.reshape(y, y_hat.shape)) ** 2 / 2 # sum이 아닌 vector 형식으로 주어집니다.\n",
    "\"\"\"\n",
    "\n",
    "# Defining the Optimization Algorithm\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "\n",
    "\"\"\"\n",
    "# Defining the Optimization Algorithm\n",
    "# params: tuple of weight and bias \n",
    "# grads: tuple of gradients w.r.t weight and bias, respectively \n",
    "# lr: learning rate \n",
    "# batch_size: size of the batch\n",
    "def sgd(params, grads, lr, batch_size): #@save\n",
    "    # Minibatch stochastic gradient descent.\n",
    "    for param, grad in zip(params, grads):\n",
    "        param.assign_sub(lr*grad/batch_size)\n",
    "\"\"\"\n",
    "\n",
    "# Training\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            l = loss(net(X, training=True), y)\n",
    "        grads = tape.gradient(l, net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    l = loss(net(features), labels) # Sample mean of the training error를 제공해주는 것 같습니다.\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}') \n",
    "\n",
    "# Discrepancy between Real Parameter Values\n",
    "w = net.get_weights()[0]\n",
    "print('error in estimating w', true_w - tf.reshape(w, true_w.shape))\n",
    "b = net.get_weights()[1]\n",
    "print('error in estimating b', true_b - b)\n",
    "\n",
    "\"\"\"\n",
    "# Training\n",
    "lr = 0.03 # learning rate\n",
    "num_epochs = 3 # number of epochs\n",
    "net = linreg # Assumption on regression ftn (modeling)\n",
    "loss = squared_loss # Loss function to optimize w.r.t. parameters\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        with tf.GradientTape() as g:\n",
    "            l = loss(net(X, w, b), y) # Minibatch loss in 'X' and 'y'\n",
    "        # Compute gradient on l w.r.t. ['w', 'b']\n",
    "        dw, db = g.gradient(l, [w, b])\n",
    "        # Update parameters using their gradient\n",
    "        sgd([w, b], [dw, db], lr, batch_size)\n",
    "    train_l = loss(net(features, w, b), labels) # Recall that loss is given in a vector\n",
    "    print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')\n",
    "\n",
    "# Discrepancy between Real Parameter Values\n",
    "print(f'error in estimating w: {true_w - tf.reshape(w, true_w.shape)}')\n",
    "print(f'error in estimating b: {true_b - b}')\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
